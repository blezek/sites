<!DOCTYPE html>
<html>
<head>

  <title>Browser head tracking</title>

  <link rel="stylesheet" href="application.css" />
  <script src='jquery-1.11.0.min.js'></script>
  <script type="text/javascript" src="numeric-1.2.6.js"></script>

</head>

<body>

<h1>Data for tracking</h1>

<h2>Be sure to approve requests for the camera(s).</h2>

<div id="canvases"></div>

<script>
//
// This is the graphics part.
// Responsible for calculating an estimate of the camera offset that generated the frames.
//

// first, the shaders
</script>

<script id="comparisonVertexShader" type="x-shader/x-vertex">

precision highp float;

// helpers
// handy refernce: http://www.3dgep.com/understanding-the-view-matrix/
mat4 perspective (float fovy, float aspect, float near, float far) {
    float f = 1.0 / tan(fovy / 2.);
    float nf = 1. / (near - far);
    mat4 perspectiveMatrix = mat4(1.);
    perspectiveMatrix[0][0] = f / aspect;
    perspectiveMatrix[1][1] = f;
    perspectiveMatrix[2][2] = (far + near) * nf;
    perspectiveMatrix[2][3] = (2. * far * near) * nf;
    perspectiveMatrix[3][3] = 0.;
    return (perspectiveMatrix);
}

mat4 lookAt (vec3 eye, vec3 direction, vec3 up) {
  vec3 x, y, z, w;
  float len;
  direction = normalize(direction);
  vec3 right = normalize(cross(direction,up));
  vec3 actualUp = normalize(cross(direction,right));

  mat4 viewMatrix = mat4(1);
  viewMatrix[0] = vec4(right,0);
  viewMatrix[1] = vec4(actualUp,0);
  viewMatrix[2] = vec4(direction,0);
  viewMatrix[3][0] = -dot(right,eye);
  viewMatrix[3][1] = -dot(actualUp,eye);
  viewMatrix[3][2] = -dot(direction,eye);

  return (viewMatrix);
}

attribute vec3 coordinate;
attribute vec2 textureCoordinate;

varying vec2 varyingTextureCoordinate;

uniform vec3 eye, direction, up;
uniform float fovy, aspect, near, far;

void main(void) {

  mat4 viewMatrix = lookAt(eye, direction, up);
  mat4 perspectiveMatrix = perspective(radians(fovy), aspect, near, far);
  gl_Position = perspectiveMatrix * viewMatrix * vec4(coordinate,1.);

  varyingTextureCoordinate = textureCoordinate;
}
</script>

<script id="comparisonFragmentShader" type="x-shader/x-fragment">
precision highp float;

uniform sampler2D referenceTextureSampler;
uniform sampler2D videoTextureSampler;
uniform vec2 windowSizeInverse;

varying vec2 varyingTextureCoordinate;

void main(void) {

  vec4 referenceRGBA = texture2D(referenceTextureSampler, vec2(1.) - gl_FragCoord.st * windowSizeInverse);
  vec4 videoRGBA = texture2D(videoTextureSampler, varyingTextureCoordinate);

  // alpha is high when images are similar
  vec4 rgba;
  rgba = videoRGBA;
  rgba.a = 1. - pow(length(referenceRGBA - videoRGBA), .6);

  gl_FragColor = rgba;
}
</script>

<script>
'use strict'
//
// Now, the rendering code
//

// add a canvas for 3D
$('#canvases').append('<canvas id=canvas3D></canvas><p id=canvas3DCaption>3D View</p>');
var canvas3D = document.querySelector('#canvas3D');

// add a canvas for 2D
$('#canvases').append('<canvas id=canvas2D></canvas><p>Reference</p>');
var canvas2D = document.querySelector('#canvas2D');
var context2D = canvas2D.getContext('2d');

//
// set up webgl
//
var gl = canvas3D.getContext('webgl');
gl.clearColor(0.0, 0.0, 0.0, 0.0);
gl.enable(gl.DEPTH_TEST);
gl.depthFunc(gl.LEQUAL); // Near things obscure far things

// create the plane vertex buffer
var planeOrigin = [0., 0.];
var planeDimensions = [1., 1.];
var planeVertices = [
  planeOrigin[0], planeOrigin[1], 0.0,
  planeOrigin[0]+planeDimensions[0], planeOrigin[1], 0.0,
  planeOrigin[0], planeOrigin[1]+planeDimensions[1], 0.0,
  planeOrigin[0]+planeDimensions[0], planeOrigin[1]+planeDimensions[1], 0.0,
];
var planeCoordinatesBuffer = gl.createBuffer();
gl.bindBuffer(gl.ARRAY_BUFFER, planeCoordinatesBuffer);
gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(planeVertices), gl.STATIC_DRAW);

var planeTextureCoordinates = [ 0, 0,  1, 0,  0, 1,  1, 1 ];
var planeTexureCoordinatesBuffer = gl.createBuffer();
gl.bindBuffer(gl.ARRAY_BUFFER, planeTexureCoordinatesBuffer);
gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(planeTextureCoordinates), gl.STATIC_DRAW);

//
// framebuffer texture for intermediate results
//
// This can be bound as the current framebuffer in order to make a
// a texture that can be the destination of a render, but then also
// read back into a typedarray for access on the CPU.
//
// http://stackoverflow.com/questions/13626606/read-pixels-from-a-webgl-texture
var scratchImage = new Image(256,256);
scratchImage.src = canvas2D.toDataURL();
var scratchArray = new Uint8Array(scratchImage.width * scratchImage.height * 4);
var scratchTexture = gl.createTexture();
var scratchFramebuffer = gl.createFramebuffer();
gl.bindTexture(gl.TEXTURE_2D, scratchTexture);
gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, scratchImage);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
gl.bindFramebuffer(gl.FRAMEBUFFER, scratchFramebuffer);
gl.framebufferTexture2D(gl.FRAMEBUFFER, gl.COLOR_ATTACHMENT0, gl.TEXTURE_2D, scratchTexture, 0);
if (gl.checkFramebufferStatus(gl.FRAMEBUFFER) != gl.FRAMEBUFFER_COMPLETE) {
  console.error('Cannot read from framebuffer texture');
}
gl.bindFramebuffer(gl.FRAMEBUFFER, null);

// create the reference texture and video texture
var referenceTextureImage = new Image();
var referenceTexture = gl.createTexture();
var setupReferenceTexture = function() {
  gl.bindTexture(gl.TEXTURE_2D, referenceTexture);
  gl.pixelStorei(gl.UNPACK_FLIP_Y_WEBGL, false);
  gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, referenceTextureImage);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
  gl.bindTexture(gl.TEXTURE_2D, null);
  var textureDimensions = [referenceTextureImage.width,referenceTextureImage.height];
};
var videoTexture = gl.createTexture();
var setupVideoTexture = function() {
  gl.bindTexture(gl.TEXTURE_2D, videoTexture);
  gl.pixelStorei(gl.UNPACK_FLIP_Y_WEBGL, false);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
  gl.bindTexture(gl.TEXTURE_2D, null);
};


//
// create the program and shaders
//
var comparisonProgram = gl.createProgram();
var comparisonVertexShader = gl.createShader(gl.VERTEX_SHADER);
gl.shaderSource(comparisonVertexShader, document.getElementById("comparisonVertexShader").innerHTML);
gl.compileShader(comparisonVertexShader);
if (!gl.getShaderParameter(comparisonVertexShader, gl.COMPILE_STATUS)) {
  alert('Could not compile comparisonVertexShader');
  console.log(gl.getShaderInfoLog(comparisonVertexShader));
}
var comparisonFragmentShader = gl.createShader(gl.FRAGMENT_SHADER);
gl.shaderSource(comparisonFragmentShader, document.getElementById("comparisonFragmentShader").innerHTML);
gl.compileShader(comparisonFragmentShader);
if (!gl.getShaderParameter(comparisonFragmentShader, gl.COMPILE_STATUS)) {
  alert('Could not compile comparisonFragmentShader');
  console.log(gl.getShaderInfoLog(comparisonFragmentShader));
}
gl.attachShader(comparisonProgram, comparisonVertexShader);
gl.deleteShader(comparisonVertexShader);
gl.attachShader(comparisonProgram, comparisonFragmentShader);
gl.deleteShader(comparisonFragmentShader);
gl.linkProgram(comparisonProgram);

// render a frame
function renderComparisonView(view) {
  gl.viewport(0, 0, canvas3D.width, canvas3D.height);
  gl.clear(gl.COLOR_BUFFER_BIT|gl.DEPTH_BUFFER_BIT);

  gl.useProgram(comparisonProgram);

  // the referenceTexture
  gl.activeTexture(gl.TEXTURE0);
  gl.bindTexture(gl.TEXTURE_2D, referenceTexture);
  gl.uniform1i(gl.getUniformLocation(comparisonProgram, "referenceTextureSampler"), 0);

  // the videoTexture
  gl.activeTexture(gl.TEXTURE1);
  gl.bindTexture(gl.TEXTURE_2D, videoTexture);
  gl.uniform1i(gl.getUniformLocation(comparisonProgram, "videoTextureSampler"), 1);

  gl.uniform2fv(gl.getUniformLocation(comparisonProgram, "windowSizeInverse"), [1./canvas3D.width, 1./canvas3D.height]);

  // the coordinate attribute
  gl.bindBuffer(gl.ARRAY_BUFFER, planeCoordinatesBuffer);
  var coordinateLocation = gl.getAttribLocation(comparisonProgram, "coordinate");
  gl.enableVertexAttribArray(coordinateLocation);
  gl.vertexAttribPointer(coordinateLocation, 3, gl.FLOAT, false, 0, 0);

  // the textureCoordinate attribute
  gl.bindBuffer(gl.ARRAY_BUFFER, planeTexureCoordinatesBuffer);
  var textureCoordinateLocation = gl.getAttribLocation(
                                      comparisonProgram, "textureCoordinate");
  gl.enableVertexAttribArray(textureCoordinateLocation);
  gl.vertexAttribPointer(textureCoordinateLocation, 2, gl.FLOAT, false, 0, 0);

  // the view
  gl.uniform3fv(gl.getUniformLocation(comparisonProgram, "eye"), view.eye);
  gl.uniform3fv(gl.getUniformLocation(comparisonProgram, "direction"), view.direction);
  gl.uniform3fv(gl.getUniformLocation(comparisonProgram, "up"), view.up);
  gl.uniform1f(gl.getUniformLocation(comparisonProgram, "fovy"), view.fovy);
  gl.uniform1f(gl.getUniformLocation(comparisonProgram, "aspect"), view.aspect);
  gl.uniform1f(gl.getUniformLocation(comparisonProgram, "near"), view.near);
  gl.uniform1f(gl.getUniformLocation(comparisonProgram, "far"), view.far);

  gl.drawArrays(gl.TRIANGLE_STRIP, 0, 4);
}

referenceTextureImage.src = "apple-keyboard.jpg";
referenceTextureImage.src = "checker.png";
referenceTextureImage.src = "NebraskaCowPasture.jpg";
referenceTextureImage.onload = setupReferenceTexture;

</script>

<p><b>TODO: factor out programs and texture etc into a render pass object
that can be reused for various calculation steps.

<p> want to be able to:
<ul>
  <li>render a series of views under different rendering parameters into the energy texture</li>
  <li>calculate the average pixel value of the energy and store it in a known location in a results texture</li>
  <li>read back results texture with readPixels and be able to map back to know which parameter set had the max energy</li>
</ul>
</b>

//
// accumulation calculations
//
<script id="accumulationVertexShader" type="x-shader/x-vertex">
precision highp float;
attribute vec3 coordinate;
attribute vec2 textureCoordinate;
varying vec2 varyingTextureCoordinate;
void main(void) {
  varyingTextureCoordinate = textureCoordinate;
  gl_Position = vec4(-1. + 2. * coordinate.x, -1. + 2. * coordinate.y/256., 0., 1.);
}
</script>

<script id="accumulationFragmentShader" type="x-shader/x-fragment">
precision highp float;
varying vec2 varyingTextureCoordinate;
uniform sampler2D sourceSampler;
const float rows = 256.; // TODO: must match scratchImage.height
void main(void) {
  vec4 accumulationRGBA = vec4(0.);
  for (float row = 0.; row < rows; row += 1.) {
    vec2 samplePoint = vec2(varyingTextureCoordinate.s, (row+.5)/rows);
    accumulationRGBA += texture2D(sourceSampler, samplePoint);
  }
  gl_FragColor = accumulationRGBA / rows;

}
</script>

<script>
'use strict'

// the accumulation texture/framebuffer is a row of pixels
// so that the comparison energy image can be reduced down
// to a single pixel energy value.
var accumulationImage = new Image(scratchImage.width,scratchImage.height);
accumulationImage.src = canvas2D.toDataURL();
var accumulationArray = new Uint8Array(accumulationImage.width * accumulationImage.height * 4);
var accumulationTexture = gl.createTexture();
var accumulationFramebuffer = gl.createFramebuffer();
gl.bindTexture(gl.TEXTURE_2D, accumulationTexture);
gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, accumulationImage);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
gl.bindFramebuffer(gl.FRAMEBUFFER, accumulationFramebuffer);
gl.framebufferTexture2D(gl.FRAMEBUFFER, gl.COLOR_ATTACHMENT0, gl.TEXTURE_2D, accumulationTexture, 0);
if (gl.checkFramebufferStatus(gl.FRAMEBUFFER) != gl.FRAMEBUFFER_COMPLETE) {
  console.error('Cannot read from framebuffer texture');
}
gl.bindFramebuffer(gl.FRAMEBUFFER, null);

// create the accumulation program and shaders
var accumulationProgram = gl.createProgram();
var accumulationVertexShader = gl.createShader(gl.VERTEX_SHADER);
gl.shaderSource(accumulationVertexShader,
    document.getElementById("accumulationVertexShader").innerHTML);
gl.compileShader(accumulationVertexShader);
if (!gl.getShaderParameter(accumulationVertexShader, gl.COMPILE_STATUS)) {
  alert('Could not compile accumulationVertexShader');
  console.log(gl.getShaderInfoLog(accumulationVertexShader));
}
var accumulationFragmentShader = gl.createShader(gl.FRAGMENT_SHADER);
gl.shaderSource(accumulationFragmentShader,
    document.getElementById("accumulationFragmentShader").innerHTML);
gl.compileShader(accumulationFragmentShader);
if (!gl.getShaderParameter(accumulationFragmentShader, gl.COMPILE_STATUS)) {
  alert('Could not compile accumulationFragmentShader');
  console.log(gl.getShaderInfoLog(accumulationFragmentShader));
}
gl.attachShader(accumulationProgram, accumulationVertexShader);
gl.deleteShader(accumulationVertexShader);
gl.attachShader(accumulationProgram, accumulationFragmentShader);
gl.deleteShader(accumulationFragmentShader);
gl.linkProgram(accumulationProgram);

function renderAccumulation(options={}) {
  gl.viewport(0, 0, scratchImage.width, scratchImage.height);
  gl.clear(gl.COLOR_BUFFER_BIT|gl.DEPTH_BUFFER_BIT);

  gl.useProgram(accumulationProgram);

  gl.activeTexture(gl.TEXTURE0);
  gl.bindTexture(gl.TEXTURE_2D, scratchTexture);
  gl.uniform1i(gl.getUniformLocation(accumulationProgram, "sourceSampler"), 0);

  gl.bindBuffer(gl.ARRAY_BUFFER, planeCoordinatesBuffer);
  var coordinateLocation = gl.getAttribLocation(accumulationProgram, "coordinate");
  gl.enableVertexAttribArray(coordinateLocation);
  gl.vertexAttribPointer(coordinateLocation, 3, gl.FLOAT, false, 0, 0);

  gl.bindBuffer(gl.ARRAY_BUFFER, planeTexureCoordinatesBuffer);
  var textureCoordinateLocation = gl.getAttribLocation(
                                    comparisonProgram, "textureCoordinate");
  gl.enableVertexAttribArray(textureCoordinateLocation);
  gl.vertexAttribPointer(textureCoordinateLocation, 2, gl.FLOAT, false, 0, 0);

  gl.drawArrays(gl.TRIANGLE_STRIP, 0, 4);
}
</script>


<span id="video"></span>
<div id='videos'> </div>
<div id='videoTime'> </div>
<script>
//
// This is the video part - it is responsible for collecting
// frames and putting them into a canvas.
//

'use strict';

var videoLabel = document.getElementById('video');
videoLabel.innerHTML = "<p> waiting for video events...</p>";

// when sources are found
function gotSources(sourceInfos) {
  // seems only one video stream can be active at a time, at least
  // on chrome on android so we pick the second video source
  // TODO allow switching sources on the fly
  var videoSourceCount = 0;
  for (var i = 0; i !== sourceInfos.length; ++i) {
    var sourceInfo = sourceInfos[i];
    if (sourceInfo.kind === 'video') {
      videoSourceCount++;
    }
  }
  var skipFirstVideo = false;
  if (videoSourceCount > 1) {
    skipFirstVideo = true;
  }
  for (var i = 0; i !== sourceInfos.length; ++i) {
    var sourceInfo = sourceInfos[i];
    if (sourceInfo.kind === 'video') {
      if (!skipFirstVideo) {
        videoLabel.innerHTML = "<p> Found some video </p>";
        videoStart(sourceInfo.id);
      }
      skipFirstVideo = false;
    }
    if (sourceInfo.kind === 'audio') {
      console.log("There's audio available, but we don't care");
    } else {
      console.log('Some other kind of source: ', sourceInfo);
    }
  }
}

// ask for sources
// getSources inspired from https://github.com/samdutton/simpl/tree/master/getusermedia
if (typeof MediaStreamTrack.getSources === 'undefined'){
  alert('This browser does not support MediaStreamTrack.\n\nTry a newer browser.');
} else {
  MediaStreamTrack.getSources(gotSources);
}

var videoCount = 0;
// play the video when it's available
function videoSuccessCallback(stream) {
  videoCount += 1;
  var videoID = "video"+videoCount;
  $('#videos').append('<video autoplay id='+videoID+'></video><p>'+videoID+'</p>');
  var videoElement = document.querySelector('#video1');
  videoElement.src = window.URL.createObjectURL(stream);
  videoElement.addEventListener('canplay', function(ev){
    var width = 512;
    var height = videoElement.videoHeight / (videoElement.videoWidth/width);
    videoElement.setAttribute('width', width);
    videoElement.setAttribute('height', height);
    canvas3D.setAttribute('width', width);
    canvas3D.setAttribute('height', height);

    renderComparisonView(optimizerView); // initial render

  }, false);
  var videoTime = document.getElementById('videoTime');
  videoElement.addEventListener('timeupdate', function() {
    videoTime.innerHTML = "<p>Time: " + videoElement.currentTime + "</p>";
  });
}

// or error out
function errorCallback(error){
  console.log('navigator.getUserMedia error: ', error);
  videoLabel.innerHTML = "<p> Error Accessing video </p>" + error;
}

function videoStart(videoSource){
  var constraints = {
    video: {
      optional: [{sourceId: videoSource}]
    }
  };
  navigator.getUserMedia(constraints, videoSuccessCallback, errorCallback);
}

var captureReferenceImage = function() {
  // draw the video frame to the canvas
  canvas2D.width = canvas3D.width;
  canvas2D.height = canvas3D.height;
  context2D.drawImage(videoElement, 0, 0, canvas2D.width, canvas2D.height);
  //let pixels = context2D.getImageData(0, 0, canvas2D.width, canvas2D.height);

  // copy a video frame to texture
  gl.bindTexture(gl.TEXTURE_2D, referenceTexture);
  gl.pixelStorei(gl.UNPACK_FLIP_Y_WEBGL, false);
  gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, videoElement);
  gl.bindTexture(gl.TEXTURE_2D, null);
}


/* rotation/quaternion utilities
    See:  http://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation#Conversion_to_and_from_the_matrix_representation

    rotation([1,0,0,0]) =>
    [[ 1.,  0.,  0.],
     [ 0.,  1.,  0.],
     [ 0.,  0.,  1.]]
*/

var rotationFromQuaternion = function(quat) {
  /* build and return a 3x3 transformation matrix from a quaternion */

  var quatNorm = Math.sqrt(quat.map(e=>e*e).reduce((a,b)=>a+b))
  var normedQuat = quat.map(e=>e/quatNorm);
  var [a,b,c,d] = normedQuat;

  var [aa, bb, cc, dd]    =  [a*a, b*b, c*c, d*d];
  var [ab, ac, ad]        =  [a*b, a*c, a*d];
  var [bc, bd, cd]        =  [b*c, b*d, c*d];

  var rot = [
    [ aa + bb - cc - dd,  2.0 * (-ad + bc),   2.0*(ac + bd)     ],
    [ 2.0 * (ad + bc),    aa - bb + cc - dd,  2.0 * (-ab + cd)  ],
    [ 2.0 * (ac + bd),    2.0 * (ab + cd),    aa - bb - cc + dd ]
  ];
  return(rot);
}

/* TODO
  def quaternion(self,Rot):
    """ extract the quaternion from a matrix (assume upper 3x3 is rotation only)
    """

    # find the permutation that is most stable (largest diagonal first)
    xx, yy, zz = abs(Rot[0,0]), abs(Rot[1,1]), abs(Rot[2,2])
    if xx > yy and xx > zz:
      u, v, w = 0, 1, 2
    else:
      if yy > xx and yy > zz:
        u, v, w = 1, 2, 0
      else:
        u, v, w = 2, 0, 1

    # fill in the quat
    r = numpy.sqrt( 1 + Rot[u,u] - Rot[v,v] - Rot[w,w] )
    r2 = 2.*r

    if r < 1e-6:
      return numpy.array([1,0,0,0])

    quat = numpy.zeros(4)
    quat[  0] = (Rot[w,v] - Rot[v,w])/r2
    quat[1+u] = r/2.
    quat[1+v] = (Rot[u,v] + Rot[v,u])/r2
    quat[1+w] = (Rot[w,u] + Rot[u,w])/r2

    return quat
*/

var referenceView = {
  eye: [0.5, 0.5, 1.],
  direction: [0., 0., 1.], // TODO: direction is flipped
  up: [0., 1, 0.],
  fovy: 45.,
  aspect: 1.,
  near: .9,
  far: 100.,
};

var viewCopy = function(view) {
  return JSON.parse(JSON.stringify(view));
}

var optimizerView = viewCopy(referenceView);

var viewDOFs = 7;
var viewWithDelta = function(view, deltaView) {
  var newView = viewCopy(view);
  var quaternion = deltaView.slice(3,7);
  quaternion[0] += 1;
  var rotation = rotationFromQuaternion(quaternion);
  for (var i = 0; i < 3; i++) {
    newView.eye[i] += deltaView[i];
    newView.up[i] = rotation[1][i];
    newView.direction[i] = rotation[2][i];
  }
  return(newView);
}

var adjustView = function(mouseEvent) {
  var rect = canvas3D.getBoundingClientRect();
  var nx = (mouseEvent.x-rect.left) / canvas3D.width;
  var ny = (mouseEvent.y-rect.top) / canvas3D.height;
  optimizerView.eye = [nx, ny, optimizerView.eye[2]];
}

var wheelView = function(mouseEvent) {
  var delta = 0.03;
  if (mouseEvent.deltaY < 0.) {
    delta *= -1;
  }
  optimizerView.eye[2] += delta;
  mouseEvent.preventDefault();
}

var viewEnergy = function(view) {

  gl.bindFramebuffer(gl.FRAMEBUFFER, scratchFramebuffer);

  renderComparisonView(view);
  // before accumulation: read whole image, now accumulate and read one line
  // gl.readPixels(0, 0, scratchImage.width, scratchImage.height, gl.RGBA, gl.UNSIGNED_BYTE, scratchArray);

  gl.bindFramebuffer(gl.FRAMEBUFFER, accumulationFramebuffer);
  renderAccumulation();
  gl.readPixels(0, 0, scratchImage.width, 1, gl.RGBA, gl.UNSIGNED_BYTE, scratchArray);

  var energy = 0.;
  var samples = 0;
  var contribution;
  for (var i = 0; i < scratchArray.length; i+=4) {
    contribution = scratchArray[i+3];
    energy += contribution;
    samples++;
  }
  gl.bindFramebuffer(gl.FRAMEBUFFER, null);
  return(energy/samples);
}

var optimizeView = function() {
  var translationStep = 0.05;
  var rotationStep = 0.005;
  var deltaSteps = Array(3).fill(translationStep).concat(Array(4).fill(rotationStep));
  var optimizationStep = 0.005;
  var lineSearchSteps = 50;
  var lineSearchStepSize = optimizationStep/lineSearchSteps;
  var dEnergydView = Array(viewDOFs).fill(0);
  var deltaView = Array(viewDOFs).fill(0);

  var savedView = viewCopy(optimizerView);
  var iterationView;
  var referenceEnergy = viewEnergy(optimizerView);
  for (var dof = 0; dof < viewDOFs; dof++) {
    deltaView[dof] = -deltaSteps[dof];
    iterationView = viewWithDelta(optimizerView, deltaView);
    var energyMinus = viewEnergy(iterationView);
    deltaView[dof] = deltaSteps[dof];
    iterationView = viewWithDelta(optimizerView, deltaView);
    var energyPlus = viewEnergy(iterationView);
    dEnergydView[dof] = (energyPlus - energyMinus) / (2 * deltaSteps[dof]);
    deltaView.fill(0);
  }

  // stepAlongGradent
  var maximumStep = {
    energy: referenceEnergy,
    deltaView: Array(viewDOFs).fill(0)
  };
  var stepView;
  for (var step = 0; step < lineSearchSteps; step += 1) {
    deltaView = dEnergydView.map(e=> e * lineSearchStepSize * step);
    stepView = viewWithDelta(optimizerView, deltaView);
    var stepEnergy = viewEnergy(stepView);
    if (stepEnergy > maximumStep.energy) {
      maximumStep.energy = stepEnergy;
      maximumStep.deltaView = deltaView.slice();
      maximumStep.step = step;
    }
  }
  optimizerView = viewWithDelta(optimizerView, maximumStep.deltaView);

  gl.bindFramebuffer(gl.FRAMEBUFFER, null);
  renderComparisonView(optimizerView);

  $('#canvas3DCaption').html(`
    <ul>
      <li>Reference energy: ${referenceEnergy}</li>
      <li>maximumStep.energy: ${maximumStep.energy}</li>
      <li>step delta: ${maximumStep.energy - referenceEnergy}</li>
      <li>maximumStep.step: ${maximumStep.step}</li>
      <li>maximumStep.deltaView: ${maximumStep.deltaView}</li>
      <li>savedView.eye: ${savedView.eye}</li>
      <li>optimizerView.eye: ${optimizerView.eye}</li>
      <li>savedView.direction: ${savedView.direction}</li>
      <li>optimizerView.direction: ${optimizerView.direction}</li>
      <li>savedView.up: ${savedView.up}</li>
      <li>optimizerView.up: ${optimizerView.up}</li>
    </ul>
  `);
}

var viewObjective = function(viewDelta) {
  return (viewEnergy(viewWithDelta(referenceView, viewDelta)));
}

canvas3D.addEventListener('click', captureReferenceImage);
canvas3D.addEventListener('mousewheel', wheelView);
canvas3D.addEventListener('mousemove', adjustView);

// the render loop to move the video into texture
var useNumeric = false;
var videoElement = null;
var videoAnimationFrame = function() {
  if (videoCount > 0) {
    if (!videoElement) {
      videoElement = document.querySelector('#video1');
      setupVideoTexture();
    } else {
      if (videoElement.videoHeight) { // only defined when loaded
        gl.bindTexture(gl.TEXTURE_2D, videoTexture);
        gl.pixelStorei(gl.UNPACK_FLIP_Y_WEBGL, false);
        //gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, videoElement);
        gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, referenceTextureImage);
        gl.bindTexture(gl.TEXTURE_2D, null);
        if (useNumeric) {
          var viewDelta = Array(viewDOFs).fill(0);
          var answer = numeric.uncmin(viewObjective, viewDelta);
          console.log(answer);
          optimizerView = viewWithDelta(referenceView, viewDelta);
          renderComparisonView(optimizerView);
        } else {
          optimizeView();
        }
      }
    }
  }
  requestAnimationFrame(videoAnimationFrame);
}
requestAnimationFrame(videoAnimationFrame);

</script>


<p>
This demo uses advanced web APIs to access sensors.  Not all devices and browsers are supported.  If you need a compatible browser try Chrome Canary.
</p>


</body>
</html>
