<!DOCTYPE html>
<html>
<head>

  <title>Browser head tracking</title>

  <link rel="stylesheet" href="application.css" />
  <script src='jquery-1.11.0.min.js'></script>

</head>

<body>

<h1>Data for tracking</h1>

<h2>Be sure to approve requests for the camera(s).</h2>

<div id="canvases"></div>

<script>
//
// This is the graphics part.
// Responsible for calculating an estimate of the camera offset that generated the frames.
//

// first, the shaders
</script>

<script id="vertexShader" type="x-shader/x-vertex">

precision highp float;

attribute vec3 coordinate;
attribute vec2 textureCoordinate;

varying vec2 varyingTextureCoordinate;

// helpers
// handy refernce: http://www.3dgep.com/understanding-the-view-matrix/
mat4 perspective (float fovy, float aspect, float near, float far) {
    float f = 1.0 / tan(fovy / 2.);
    float nf = 1. / (near - far);
    mat4 perspectiveMatrix = mat4(1.);
    perspectiveMatrix[0][0] = f / aspect;
    perspectiveMatrix[1][1] = f;
    perspectiveMatrix[2][2] = (far + near) * nf;
    perspectiveMatrix[2][3] = (2. * far * near) * nf;
    perspectiveMatrix[3][3] = 0.;
    return (perspectiveMatrix);
}

mat4 lookAt (vec3 eye, vec3 center, vec3 up) {
  vec3 x, y, z, w;
  float len;
  vec3 direction = normalize(eye - center);
  vec3 right = normalize(cross(direction,up));
  vec3 actualUp = normalize(cross(direction,right));

  mat4 viewMatrix = mat4(1);
  viewMatrix[0] = vec4(right,0);
  viewMatrix[1] = vec4(actualUp,0);
  viewMatrix[2] = vec4(direction,0);
  viewMatrix[3][0] = -dot(right,eye);
  viewMatrix[3][1] = -dot(actualUp,eye);
  viewMatrix[3][2] = -dot(direction,eye);

  return (viewMatrix);
}

void main(void) {

  //mat4 viewMatrix = lookAt(vec3(0, 50, 55), vec3(5, 5, 0), vec3(0, 1, 0));
  mat4 viewMatrix = lookAt(vec3(0., 0., 60.), vec3(0, 0, 0), vec3(0, 1, 0));

  mat4 perspectiveMatrix = perspective(radians(45.), 1., 1., 100.);

  gl_Position = perspectiveMatrix * viewMatrix * vec4(coordinate,1.);

  varyingTextureCoordinate = textureCoordinate;
}
</script>

<script id="fragmentShader" type="x-shader/x-fragment">
precision highp float;

uniform sampler2D referenceTextureSampler;
uniform sampler2D videoTextureSampler;

varying vec2 varyingTextureCoordinate;

void main(void) {

  vec4 referenceRGBA = texture2D(referenceTextureSampler, varyingTextureCoordinate);
  vec4 videoRGBA = texture2D(videoTextureSampler, varyingTextureCoordinate);

  vec4 rgba;

  rgba = referenceRGBA;
  rgba = 0.5 * (referenceRGBA + videoRGBA);
  rgba = abs(referenceRGBA - videoRGBA);
  rgba.a = 1.;

  gl_FragColor = rgba;
}
</script>

<script>
//
// Now, the rendering code
//

// add a canvas for 3D
$('#canvases').append('<canvas id=canvas3D></canvas><p>3D View</p>');
var canvas3D = document.querySelector('#canvas3D');

// add a canvas for 2D
$('#canvases').append('<canvas id=canvas2D></canvas><p>Reference</p>');
var canvas2D = document.querySelector('#canvas2D');
var context2D = canvas2D.getContext('2d');

'use strict'

//
// set up webgl
//
var gl = canvas3D.getContext('webgl');
gl.clearColor(0.0, 0.0, 0.0, 1.0); // black, fully opaque
gl.enable(gl.DEPTH_TEST);
gl.depthFunc(gl.LEQUAL); // Near things obscure far things

// create the plane vertex buffer
var planeOrigin = [-60., -60.];
var planeDimensions = [120., 120.];
//var planeOrigin = [-1., -1.];
//var planeDimensions = [2., 2.];
var planeVertices = [
  planeOrigin[0], planeOrigin[1], 0.0,
  planeOrigin[0]+planeDimensions[0], planeOrigin[1], 0.0,
  planeOrigin[0], planeOrigin[1]+planeDimensions[1], 0.0,
  planeOrigin[0]+planeDimensions[0], planeOrigin[1]+planeDimensions[1], 0.0,
];
var planeCoordinatesBuffer = gl.createBuffer();
gl.bindBuffer(gl.ARRAY_BUFFER, planeCoordinatesBuffer);
gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(planeVertices), gl.STATIC_DRAW);

var planeAspect = 1024 / 576;
var planeTextureCoordinates = [ 0, 0,  1, 0,  0, 1,  1, 1 ];
var planeTexureCoordinatesBuffer = gl.createBuffer();
gl.bindBuffer(gl.ARRAY_BUFFER, planeTexureCoordinatesBuffer);
gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(planeTextureCoordinates), gl.STATIC_DRAW);

// create the reference texture and video texture
var referenceTextureImage = new Image();
var referenceTexture = gl.createTexture();
var setupReferenceTexture = function() {
  gl.bindTexture(gl.TEXTURE_2D, referenceTexture);
  gl.pixelStorei(gl.UNPACK_FLIP_Y_WEBGL, false);
  gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, referenceTextureImage);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
  gl.bindTexture(gl.TEXTURE_2D, null);
  var textureDimensions = [referenceTextureImage.width,referenceTextureImage.height];
};
var videoTexture = gl.createTexture();
var setupVideoTexture = function() {
  gl.bindTexture(gl.TEXTURE_2D, videoTexture);
  gl.pixelStorei(gl.UNPACK_FLIP_Y_WEBGL, false);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
  gl.bindTexture(gl.TEXTURE_2D, null);
};

//
// create the program and shaders
//
var glProgram = gl.createProgram();
var vertexShader = gl.createShader(gl.VERTEX_SHADER);
gl.shaderSource(vertexShader, document.getElementById("vertexShader").innerHTML);
gl.compileShader(vertexShader);
if (!gl.getShaderParameter(vertexShader, gl.COMPILE_STATUS)) {
  alert('Could not compile vertexShader');
  console.log(gl.getShaderInfoLog(vertexShader));
}
var fragmentShader = gl.createShader(gl.FRAGMENT_SHADER);
gl.shaderSource(fragmentShader, document.getElementById("fragmentShader").innerHTML);
gl.compileShader(fragmentShader);
if (!gl.getShaderParameter(fragmentShader, gl.COMPILE_STATUS)) {
  alert('Could not compile fragmentShader');
  console.log(gl.getShaderInfoLog(fragmentShader));
}
gl.attachShader(glProgram, vertexShader);
gl.deleteShader(vertexShader);
gl.attachShader(glProgram, fragmentShader);
gl.deleteShader(fragmentShader);
gl.linkProgram(glProgram);



// render a frame
function render3D() {
  gl.viewport(0, 0, canvas3D.width, canvas3D.height);
  gl.clear(gl.COLOR_BUFFER_BIT|gl.DEPTH_BUFFER_BIT);

  gl.useProgram(glProgram);

  // the referenceTexture
  gl.activeTexture(gl.TEXTURE0);
  gl.bindTexture(gl.TEXTURE_2D, referenceTexture);
  gl.uniform1i(gl.getUniformLocation(glProgram, "referenceTextureSampler"), 0);

  // the videoTexture
  gl.activeTexture(gl.TEXTURE1);
  gl.bindTexture(gl.TEXTURE_2D, videoTexture);
  gl.uniform1i(gl.getUniformLocation(glProgram, "videoTextureSampler"), 1);

  // the coordinate attribute
  gl.bindBuffer(gl.ARRAY_BUFFER, planeCoordinatesBuffer);
  var coordinateLocation = gl.getAttribLocation(glProgram, "coordinate");
  gl.enableVertexAttribArray( gl.getAttribLocation(glProgram, "coordinate") );
  gl.vertexAttribPointer( coordinateLocation, 3, gl.FLOAT, false, 0, 0);

  // the textureCoordinate attribute
  gl.bindBuffer(gl.ARRAY_BUFFER, planeTexureCoordinatesBuffer);
  var textureCoordinateLocation = gl.getAttribLocation(glProgram, "textureCoordinate");
  gl.enableVertexAttribArray( gl.getAttribLocation(glProgram, "textureCoordinate") );
  gl.vertexAttribPointer( textureCoordinateLocation, 2, gl.FLOAT, false, 0, 0);

  gl.drawArrays(gl.TRIANGLE_STRIP, 0, 4);
}

referenceTextureImage.src = "apple-keyboard.jpg";
referenceTextureImage.onload = setupReferenceTexture;

</script>


<span id="video"></span>
<div id='videos'> </div>
<div id='videoTime'> </div>
<script>
//
// This is the video part - it is responsible for collecting
// frames and putting them into a canvas.
//

'use strict';

var videoLabel = document.getElementById('video');
videoLabel.innerHTML = "<p> waiting for video events...</p>";

// when sources are found
function gotSources(sourceInfos) {
  // seems only one video stream can be active at a time, at least
  // on chrome on android so we pick the second video source
  // TODO allow switching sources on the fly
  var videoSourceCount = 0;
  for (var i = 0; i !== sourceInfos.length; ++i) {
    var sourceInfo = sourceInfos[i];
    if (sourceInfo.kind === 'video') {
      videoSourceCount++;
    }
  }
  var skipFirstVideo = false;
  if (videoSourceCount > 1) {
    skipFirstVideo = true;
  }
  for (var i = 0; i !== sourceInfos.length; ++i) {
    var sourceInfo = sourceInfos[i];
    if (sourceInfo.kind === 'video') {
      if (!skipFirstVideo) {
        videoLabel.innerHTML = "<p> Found some video </p>";
        videoStart(sourceInfo.id);
      }
      skipFirstVideo = false;
    }
    if (sourceInfo.kind === 'audio') {
      console.log("There's audio available, but we don't care");
    } else {
      console.log('Some other kind of source: ', sourceInfo);
    }
  }
}

// ask for sources
// getSources inspired from https://github.com/samdutton/simpl/tree/master/getusermedia
if (typeof MediaStreamTrack.getSources === 'undefined'){
  alert('This browser does not support MediaStreamTrack.\n\nTry a newer browser.');
} else {
  MediaStreamTrack.getSources(gotSources);
}

var videoCount = 0;
// play the video when it's available
function videoSuccessCallback(stream) {
  videoCount += 1;
  var videoID = "video"+videoCount;
  $('#videos').append('<video autoplay id='+videoID+'></video><p>'+videoID+'</p>');
  var videoElement = document.querySelector('#video1');
  videoElement.src = window.URL.createObjectURL(stream);
  videoElement.addEventListener('canplay', function(ev){
    var width = 400;
    var height = videoElement.videoHeight / (videoElement.videoWidth/width);
    videoElement.setAttribute('width', width);
    videoElement.setAttribute('height', height);
    canvas3D.setAttribute('width', width);
    canvas3D.setAttribute('height', height);

    render3D(); // initial render

  }, false);
  var videoTime = document.getElementById('videoTime');
  videoElement.addEventListener('timeupdate', function() {
    videoTime.innerHTML = "<p>Time: " + videoElement.currentTime + "</p>";
  });
}

// or error out
function errorCallback(error){
  console.log('navigator.getUserMedia error: ', error);
  videoLabel.innerHTML = "<p> Error Accessing video </p>" + error;
}

function videoStart(videoSource){
  var constraints = {
    video: {
      optional: [{sourceId: videoSource}]
    }
  };
  navigator.getUserMedia(constraints, videoSuccessCallback, errorCallback);
}

var captureReferenceImage = function() {
  canvas2D.width = canvas3D.width;
  canvas2D.height = canvas3D.height;
  context2D.drawImage(videoElement, 0, 0, canvas2D.width, canvas2D.height);
  //let pixels = context2D.getImageData(0, 0, canvas2D.width, canvas2D.height);

  gl.bindTexture(gl.TEXTURE_2D, referenceTexture);
  gl.pixelStorei(gl.UNPACK_FLIP_Y_WEBGL, false);
  gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, videoElement);
  gl.bindTexture(gl.TEXTURE_2D, null);
}
canvas3D.addEventListener('click', captureReferenceImage);

// the render loop to move the video into texture
var videoElement = null;
var videoAnimationFrame = function() {
  if (videoCount > 0) {
    if (!videoElement) {
      videoElement = document.querySelector('#video1');
      setupVideoTexture();
    } else {
      if (videoElement.videoHeight) { // only defined when loaded

          gl.bindTexture(gl.TEXTURE_2D, videoTexture);
          gl.pixelStorei(gl.UNPACK_FLIP_Y_WEBGL, false);
          gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, videoElement);
          gl.bindTexture(gl.TEXTURE_2D, null);
        for(let i = 0; i < 1000; i++) {
          render3D();
        }
      }
    }
  }
  requestAnimationFrame(videoAnimationFrame);
}

requestAnimationFrame(videoAnimationFrame);
</script>


<p>
This demo uses advanced web APIs to access sensors.  Not all devices and browsers are supported.  If you need a compatible browser try Chrome Canary.
</p>


</body>
</html>
